#!/bin/bash

# ä¿®æ­£ç‰ˆæŽ¨ç†è„šæœ¬ - æ­£ç¡®å¤„ç† verl åŽŸç”Ÿæ£€æŸ¥ç‚¹
# éœ€è¦åœ¨ AutoDL æœåŠ¡å™¨ä¸Šè¿è¡Œ

cd /root/autodl-tmp/verl
source verl_env/bin/activate

export HF_ENDPOINT=https://hf-mirror.com
export WANDB_MODE=offline

echo "=== æ£€æŸ¥ç‚¹ç»“æž„åˆ†æž ==="

# 1. æŸ¥æ‰¾æ£€æŸ¥ç‚¹
CHECKPOINT_BASE="/root/autodl-tmp/verl"
LATEST_CHECKPOINT=$(find $CHECKPOINT_BASE -name "global_step_*" -type d | sort -V | tail -1)

if [ -z "$LATEST_CHECKPOINT" ]; then
    echo "âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡åž‹"
    MODEL_PATH="Qwen/Qwen2.5-0.5B-Instruct"
    USE_CHECKPOINT=false
else
    echo "âœ… æ‰¾åˆ°æ£€æŸ¥ç‚¹: $LATEST_CHECKPOINT"
    
    # 2. è¯¦ç»†æŸ¥çœ‹æ£€æŸ¥ç‚¹ç»“æž„
    echo "æ£€æŸ¥ç‚¹ç›®å½•ç»“æž„:"
    ls -la "$LATEST_CHECKPOINT" | sed 's/^/  /'
    
    if [ -d "$LATEST_CHECKPOINT/actor" ]; then
        echo "Actor ç›®å½•ç»“æž„:"
        ls -la "$LATEST_CHECKPOINT/actor" | sed 's/^/    /'
    fi
    
    # 3. æ£€æŸ¥ä¸åŒçš„å¯èƒ½è·¯å¾„
    POSSIBLE_PATHS=(
        "$LATEST_CHECKPOINT/actor/huggingface"
        "$LATEST_CHECKPOINT/actor"
        "$LATEST_CHECKPOINT/huggingface"
        "$LATEST_CHECKPOINT"
    )
    
    MODEL_PATH=""
    USE_CHECKPOINT=false
    
    for path in "${POSSIBLE_PATHS[@]}"; do
        if [ -f "$path/config.json" ] && [ -f "$path/pytorch_model.bin" -o -f "$path/model.safetensors" ]; then
            echo "âœ… æ‰¾åˆ° HuggingFace æ ¼å¼æ¨¡åž‹: $path"
            MODEL_PATH="$path"
            USE_CHECKPOINT=true
            break
        elif [ -f "$path/config.json" ]; then
            echo "âš ï¸  æ‰¾åˆ°é…ç½®æ–‡ä»¶ä½†ç¼ºå°‘æ¨¡åž‹æ–‡ä»¶: $path"
        fi
    done
    
    if [ "$USE_CHECKPOINT" = false ]; then
        echo "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„ HuggingFace æ ¼å¼æ¨¡åž‹"
        echo "ðŸ“ æ£€æŸ¥ç‚¹å†…å®¹è¯¦æƒ…:"
        find "$LATEST_CHECKPOINT" -type f -name "*.json" -o -name "*.bin" -o -name "*.safetensors" | head -10 | sed 's/^/    /'
        
        # 4. å°è¯•ä½¿ç”¨ verl åŽŸç”ŸæŽ¨ç†
        echo ""
        echo "ðŸ”§ å°è¯•ä½¿ç”¨ verl åŽŸç”Ÿæ£€æŸ¥ç‚¹æ ¼å¼è¿›è¡ŒæŽ¨ç†..."
        MODEL_PATH="$LATEST_CHECKPOINT"
        USE_VERL_NATIVE=true
    fi
fi

echo "æœ€ç»ˆä½¿ç”¨æ¨¡åž‹è·¯å¾„: $MODEL_PATH"
echo "ä½¿ç”¨æ£€æŸ¥ç‚¹: $USE_CHECKPOINT"
echo ""

# 5. åˆ›å»ºæµ‹è¯•æ•°æ®
cat > /tmp/test_prompts.json << 'EOF'
[
    [{"role": "user", "content": "Solve this step by step: If a train travels 120 miles in 2 hours, what is its average speed?"}],
    [{"role": "user", "content": "Calculate: What is 15% of 240?"}],
    [{"role": "user", "content": "A rectangle has length 8 cm and width 5 cm. What is its area and perimeter?"}],
    [{"role": "user", "content": "If x + 5 = 12, what is the value of x?"}],
    [{"role": "user", "content": "Sarah has 24 apples. She gives away 1/3 of them. How many apples does she have left?"}]
]
EOF

python3 << 'EOF'
import json
import pandas as pd

with open('/tmp/test_prompts.json', 'r') as f:
    prompts = json.load(f)

df = pd.DataFrame({'prompt': prompts})
df.to_parquet('/tmp/test_data.parquet', index=False)
print(f"Created test dataset with {len(df)} samples")
EOF

OUTPUT_PATH="/root/autodl-tmp/verl/inference_results.parquet"

# 6. æ ¹æ®æ£€æŸ¥ç‚¹ç±»åž‹é€‰æ‹©æŽ¨ç†æ–¹å¼
if [ "$USE_CHECKPOINT" = true ]; then
    echo "ðŸš€ ä½¿ç”¨ HuggingFace æ ¼å¼æ£€æŸ¥ç‚¹è¿›è¡ŒæŽ¨ç†..."
    
    python3 -m verl.trainer.main_generation \
        trainer.nnodes=1 \
        trainer.n_gpus_per_node=1 \
        data.path=/tmp/test_data.parquet \
        data.prompt_key=prompt \
        data.n_samples=1 \
        data.batch_size=5 \
        data.output_path=$OUTPUT_PATH \
        model.path=$MODEL_PATH \
        rollout.name=vllm \
        rollout.temperature=0.7 \
        rollout.top_k=-1 \
        rollout.top_p=0.9 \
        rollout.prompt_length=512 \
        rollout.response_length=256 \
        rollout.tensor_model_parallel_size=1 \
        rollout.gpu_memory_utilization=0.8

elif [ -n "$USE_VERL_NATIVE" ]; then
    echo "ðŸ”§ å°è¯•ä½¿ç”¨ verl åŽŸç”Ÿæ ¼å¼..."
    echo "âš ï¸  è¿™éœ€è¦ç‰¹æ®Šçš„åŠ è½½æ–¹å¼ï¼Œæš‚æ—¶ä½¿ç”¨åŸºç¡€æ¨¡åž‹è¿›è¡Œå¯¹æ¯”æµ‹è¯•"
    
    # ä½¿ç”¨åŸºç¡€æ¨¡åž‹ä½œä¸ºå¯¹æ¯”
    python3 -m verl.trainer.main_generation \
        trainer.nnodes=1 \
        trainer.n_gpus_per_node=1 \
        data.path=/tmp/test_data.parquet \
        data.prompt_key=prompt \
        data.n_samples=1 \
        data.batch_size=5 \
        data.output_path=$OUTPUT_PATH \
        model.path="Qwen/Qwen2.5-0.5B-Instruct" \
        rollout.name=vllm \
        rollout.temperature=0.7 \
        rollout.top_k=-1 \
        rollout.top_p=0.9 \
        rollout.prompt_length=512 \
        rollout.response_length=256 \
        rollout.tensor_model_parallel_size=1 \
        rollout.gpu_memory_utilization=0.8
        
    echo ""
    echo "ðŸ“‹ ä¸‹é¢æ˜¯è½¬æ¢æ£€æŸ¥ç‚¹ä¸º HuggingFace æ ¼å¼çš„æ–¹æ³•:"
    echo "1. æŸ¥çœ‹ verl è®­ç»ƒé…ç½®ä¸­çš„ trainer.save_hf_model é€‰é¡¹"
    echo "2. æˆ–è€…è¿è¡Œä¸“é—¨çš„è½¬æ¢è„šæœ¬"
    
else
    echo "ðŸŽ¯ ä½¿ç”¨åŸºç¡€æ¨¡åž‹è¿›è¡ŒæŽ¨ç†..."
    
    python3 -m verl.trainer.main_generation \
        trainer.nnodes=1 \
        trainer.n_gpus_per_node=1 \
        data.path=/tmp/test_data.parquet \
        data.prompt_key=prompt \
        data.n_samples=1 \
        data.batch_size=5 \
        data.output_path=$OUTPUT_PATH \
        model.path="Qwen/Qwen2.5-0.5B-Instruct" \
        rollout.name=vllm \
        rollout.temperature=0.7 \
        rollout.top_k=-1 \
        rollout.top_p=0.9 \
        rollout.prompt_length=512 \
        rollout.response_length=256 \
        rollout.tensor_model_parallel_size=1 \
        rollout.gpu_memory_utilization=0.8
fi

echo ""
echo "=== æŽ¨ç†å®Œæˆ ==="

if [ -f "$OUTPUT_PATH" ]; then
    echo "âœ… ç»“æžœä¿å­˜åˆ°: $OUTPUT_PATH"
    
    python3 << EOF
import pandas as pd
import json

df = pd.read_parquet('$OUTPUT_PATH')

print("=== æŽ¨ç†ç»“æžœ ===")
for i, row in df.iterrows():
    print(f"\né—®é¢˜ {i+1}:")
    print(f"Q: {row['prompt'][0]['content']}")
    print(f"A: {row['responses'][0]}")
    print("-" * 50)

# ä¿å­˜ä¸º JSON
results = []
for i, row in df.iterrows():
    results.append({
        "question": row['prompt'][0]['content'],
        "answer": row['responses'][0]
    })

with open('/root/autodl-tmp/verl/inference_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"\nç»“æžœå·²ä¿å­˜:")
print(f"- Parquet: $OUTPUT_PATH")
print(f"- JSON: /root/autodl-tmp/verl/inference_results.json")
EOF
else
    echo "âŒ æŽ¨ç†å¤±è´¥ï¼Œæœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
fi

# æ¸…ç†
rm -f /tmp/test_prompts.json /tmp/test_data.parquet

echo ""
echo "=== æ€»ç»“ ==="
if [ "$USE_CHECKPOINT" = true ]; then
    echo "âœ… æˆåŠŸä½¿ç”¨è®­ç»ƒåŽçš„æ£€æŸ¥ç‚¹è¿›è¡ŒæŽ¨ç†"
elif [ -n "$USE_VERL_NATIVE" ]; then
    echo "âš ï¸  æ£€æŸ¥ç‚¹å­˜åœ¨ä½†éœ€è¦è½¬æ¢ä¸º HuggingFace æ ¼å¼"
    echo "ðŸ’¡ å»ºè®®ï¼šé‡æ–°è®­ç»ƒæ—¶è®¾ç½®ä¿å­˜ HuggingFace æ ¼å¼"
else
    echo "â„¹ï¸  ä½¿ç”¨åŸºç¡€æ¨¡åž‹è¿›è¡ŒæŽ¨ç†ï¼ˆæœªæ‰¾åˆ°å¯ç”¨æ£€æŸ¥ç‚¹ï¼‰"
fi 